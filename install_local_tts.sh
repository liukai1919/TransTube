#!/bin/bash

# TransTube æœ¬åœ° TTS å®‰è£…è„šæœ¬
# å®‰è£… Aâ†’Bâ†’Câ†’D ç®¡é“æ‰€éœ€çš„æœ¬åœ°æ¨¡å‹

echo "ğŸš€ å¼€å§‹å®‰è£… TransTube æœ¬åœ°è¯­éŸ³ç¿»è¯‘ä¾èµ–..."

# æ£€æŸ¥ Python ç‰ˆæœ¬
python_version=$(python3 -c 'import sys; print(".".join(map(str, sys.version_info[:2])))')
echo "æ£€æµ‹åˆ° Python ç‰ˆæœ¬: $python_version"

# æ£€æŸ¥ Python ç‰ˆæœ¬æ˜¯å¦æ»¡è¶³è¦æ±‚
if [ "$(printf '%s\n' "3.8" "$python_version" | sort -V | head -n1)" != "3.8" ]; then
    echo "âŒ é”™è¯¯: éœ€è¦ Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬"
    exit 1
fi

# æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨
if command -v nvidia-smi &> /dev/null; then
    echo "âœ… æ£€æµ‹åˆ° NVIDIA GPU"
    nvidia-smi
else
    echo "âš ï¸ æœªæ£€æµ‹åˆ° NVIDIA GPUï¼Œå°†ä½¿ç”¨ CPU æ¨¡å¼"
fi

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
echo "ğŸ“¦ åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ..."
python3 -m venv venv
source venv/bin/activate

# å‡çº§ pip
echo "ğŸ“¦ å‡çº§ pip..."
pip install --upgrade pip

# å®‰è£… PyTorch
echo "ğŸ“¦ å®‰è£… PyTorch..."
if command -v nvidia-smi &> /dev/null; then
    # å¦‚æœæœ‰ GPUï¼Œå®‰è£… CUDA ç‰ˆæœ¬
    pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121
else
    # å¦‚æœæ²¡æœ‰ GPUï¼Œå®‰è£… CPU ç‰ˆæœ¬
    pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121
fi

# å®‰è£…æœ¬åœ° TTS ä¾èµ–
echo "ğŸ“¦ å®‰è£…æœ¬åœ° TTS ä¾èµ–..."
pip install -r backend/requirements_local_tts.txt

# ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
echo "ğŸ“¥ ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹..."

# åˆ›å»ºæ¨¡å‹ç¼“å­˜ç›®å½•
mkdir -p ~/.cache/whisper
mkdir -p ~/.cache/huggingface
mkdir -p ~/.cache/tts

# è®¾ç½® Whisper æ¨¡å‹å¤§å°ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼Œé»˜è®¤ä¸º large-v3
WHISPER_MODEL_SIZE=${WHISPER_MODEL_SIZE:-large-v3}

# å¦‚æœç”¨æˆ·åœ¨è°ƒç”¨è„šæœ¬æ—¶æŒ‡å®šäº† "--tiny|--base|--small|--medium|--large|--large-v2|--large-v3" ç­‰æ ‡å¿—ï¼Œä¹Ÿåšå…¼å®¹å¤„ç†
for arg in "$@"; do
  case $arg in
    --tiny|--base|--small|--medium|--large|--large-v2|--large-v3)
      WHISPER_MODEL_SIZE=${arg#--}
      shift
      ;;
  esac
 done

echo "ğŸ“¥ ä¸‹è½½ faster-whisper æ¨¡å‹..."
python3 -c "
from faster_whisper import WhisperModel
import os, sys
size = os.getenv('WHISPER_MODEL_SIZE', '$WHISPER_MODEL_SIZE')
print(f'æ­£åœ¨ä¸‹è½½ faster-whisper {size} æ¨¡å‹...')
model = WhisperModel(size, device='cpu', compute_type='int8')
print('faster-whisper æ¨¡å‹ä¸‹è½½å®Œæˆ')
"

echo "ğŸ“¥ ä¸‹è½½ NLLB ç¿»è¯‘æ¨¡å‹..."
python3 -c "
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
print('æ­£åœ¨ä¸‹è½½ NLLB-200-distilled-1.3B æ¨¡å‹...')
model_name = 'facebook/nllb-200-distilled-1.3B'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
print('NLLB ç¿»è¯‘æ¨¡å‹ä¸‹è½½å®Œæˆ')
"

echo "ğŸ“¥ ä¸‹è½½ TTS æ¨¡å‹..."
python3 -c "
from TTS.api import TTS
print('æ­£åœ¨ä¸‹è½½ä¸­æ–‡ TTS æ¨¡å‹...')
tts = TTS(model_name='tts_models/zh-CN/baker/tacotron2-DDC-GST', progress_bar=True)
print('TTS æ¨¡å‹ä¸‹è½½å®Œæˆ')
"

# æµ‹è¯•å®‰è£…
echo "ğŸ§ª æµ‹è¯•å®‰è£…..."
python3 -c "
import sys
sys.path.append('backend')

try:
    from utils.local_tts import LocalSpeechTranslationPipeline
    print('âœ… æœ¬åœ° TTS æ¨¡å—å¯¼å…¥æˆåŠŸ')
    
    # åˆ›å»ºç®¡é“å®ä¾‹ï¼ˆä¸åˆå§‹åŒ–æ¨¡å‹ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…ï¼‰
    print('âœ… æ‰€æœ‰ä¾èµ–å®‰è£…æˆåŠŸ')
    
except Exception as e:
    print(f'âŒ æµ‹è¯•å¤±è´¥: {str(e)}')
    sys.exit(1)
"

# æ›´æ–°ç¯å¢ƒå˜é‡
echo "âš™ï¸  é…ç½®ç¯å¢ƒå˜é‡..."
if [ ! -f .env ]; then
    cp env.example .env
fi

# æ·»åŠ æœ¬åœ° TTS é…ç½®
if ! grep -q "USE_LOCAL_TTS" .env; then
    echo "" >> .env
    echo "# æœ¬åœ° TTS é…ç½®" >> .env
    echo "USE_LOCAL_TTS=true" >> .env
    echo "LOCAL_TTS_MODEL=tts_models/zh-CN/baker/tacotron2-DDC-GST" >> .env
    echo "WHISPER_MODEL_SIZE=$WHISPER_MODEL_SIZE" >> .env
    echo "TRANSLATION_MODEL=nllb-200-distilled-1.3B" >> .env
fi

# æ›´æ–° WHISPER_MODEL_SIZE å¦‚æœå·²å­˜åœ¨æ¡ç›®
if grep -q "WHISPER_MODEL_SIZE=" .env; then
    sed -i "s/^WHISPER_MODEL_SIZE=.*/WHISPER_MODEL_SIZE=$WHISPER_MODEL_SIZE/" .env
fi

echo "âœ… æœ¬åœ° TTS å®‰è£…å®Œæˆï¼"
echo ""
echo "ğŸ“‹ å®‰è£…æ‘˜è¦:"
echo "- Python ç‰ˆæœ¬: $python_version"
echo "- PyTorch: $(python3 -c 'import torch; print(torch.__version__)')"
echo "- CUDA å¯ç”¨: $(if command -v nvidia-smi &> /dev/null; then echo 'æ˜¯'; else echo 'å¦'; fi)"
echo "- æ¨¡å‹ç¼“å­˜ç›®å½•:"
echo "  - Whisper: ~/.cache/whisper"
echo "  - HuggingFace: ~/.cache/huggingface"
echo "  - TTS: ~/.cache/tts"
echo ""
echo "ğŸš€ ç°åœ¨ä½ å¯ä»¥ä½¿ç”¨æœ¬åœ° TTS åŠŸèƒ½äº†ï¼"

# æ˜¾ç¤ºç£ç›˜ä½¿ç”¨æƒ…å†µ
echo ""
echo "ğŸ’¾ æ¨¡å‹æ–‡ä»¶å¤§å°ä¼°ç®—:"
echo "   - faster-whisper large-v3: ~1.5GB"
echo "   - NLLB-200-distilled-1.3B: ~2.5GB"
echo "   - TTS æ¨¡å‹: ~200MB"
echo "   - æ€»è®¡: ~4.2GB" 