# -*- coding: utf-8 -*-
"""
å­—å¹•ç¿»è¯‘æ¨¡å—ï¼ˆOllama ç‰ˆæœ¬ï¼Œæç®€å®ç°ï¼‰

ä¾èµ–è¿è¡Œåœ¨æœ¬åœ°æˆ–å±€åŸŸç½‘çš„ **Ollama** æœåŠ¡è¿›è¡Œ Chat äº¤äº’ï¼Œ
ä¿ç•™ translate_srt_to_zh(srt_path, target_lang="zh") åŠ translate_text æ¥å£ï¼Œ
å…¼å®¹ main.py / processor.py ç°æœ‰è°ƒç”¨ã€‚

å·¥ä½œæµç¨‹ï¼š
1. è§£æè¾“å…¥ SRTï¼Œæå–æ‰€æœ‰è¡Œæ–‡æœ¬ã€‚
2. æŒ‰å­—ç¬¦é˜ˆå€¼ (é»˜è®¤ 3500) åˆ†æ‰¹æäº¤ç»™ Ollama Chat APIï¼Œæ¯æ‰¹è¿”å›é€è¡Œè¯‘æ–‡ã€‚
3. å°†è¯‘æ–‡å†™å›å­—å¹•æ¡ç›®å¹¶ç”Ÿæˆæ–°çš„ SRT æ–‡ä»¶ã€‚

ç¯å¢ƒå˜é‡ï¼š
OLLAMA_URL  (é»˜è®¤ "http://localhost:11434")
OLLAMA_MODEL (é»˜è®¤ "gpt-oss:20b")
OLLAMA_NUM_PREDICT (é»˜è®¤ 1024)
TRANSLATE_BATCH_CHAR_LIMIT (é»˜è®¤ 3500)
"""

from __future__ import annotations

import os
import logging
import tempfile
from typing import List

import requests  # ä¸æœ¬åœ° Ollama é€šè®¯
import json
import srt       # python-srt è§£æåº“
from pathlib import Path

# ------------------------- ç¯å¢ƒå˜é‡ ------------------------- #
# Provider é€‰æ‹©ï¼šollama | openaiï¼ˆOpenAI API å…¼å®¹ï¼‰
TRANSLATE_PROVIDER = os.getenv("TRANSLATE_PROVIDER", "ollama").lower()
TRANSLATE_LINE_BY_LINE = os.getenv("TRANSLATE_LINE_BY_LINE", "0").lower() in {"1", "true", "yes"}

# Ollamaï¼ˆæœ¬åœ°/å…¼å®¹ï¼‰
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "gemma3:27b")
# å¤‡ç”¨æ¨¡å‹ï¼šå½“ä¸»æ¨¡å‹è¾“å‡ºä¸ºç©ºæˆ–æ— ä¸­æ–‡æ—¶ç”¨äºäºŒæ¬¡é‡è¯•ï¼ˆå¯é€‰ï¼‰
OLLAMA_FALLBACK_MODEL = os.getenv("OLLAMA_FALLBACK_MODEL", "").strip()
NUM_PREDICT = int(os.getenv("OLLAMA_NUM_PREDICT", "1024"))
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")

# OpenAI å…¼å®¹ï¼ˆå¦‚ OpenAIã€DeepSeek ç­‰ï¼‰
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", os.getenv("OLLAMA_MODEL", "gpt-4o-mini"))
OPENAI_MAX_TOKENS = int(os.getenv("OPENAI_MAX_TOKENS", "1024"))

# æ‰¹é‡åˆ†ç‰‡
BATCH_CHAR_LIMIT = int(os.getenv("TRANSLATE_BATCH_CHAR_LIMIT", "500"))

# æ‰¹é‡ç¿»è¯‘å®šç•Œç¬¦ï¼Œæ¨¡å‹å‡ ä¹ä¸ä¼šç”Ÿæˆè¯¥ä¸²
DELIM = "<<<|||>>>"

# ------------------------- æ—¥å¿— ------------------------- #
logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)
# ------------------------- æœ¯è¯­è¯è¡¨ ------------------------- #
TERMINOLOGY_FILE = str(Path(__file__).resolve().parent.parent / "config" / "terminology.txt")

def _load_keep_terms() -> list:
    terms: list[str] = []
    try:
        with open(TERMINOLOGY_FILE, "r", encoding="utf-8") as fp:
            for ln in fp:
                t = ln.strip()
                if not t or t.startswith("#"):
                    continue
                terms.append(t)
    except Exception:
        pass
    return terms

KEEP_TERMS = _load_keep_terms()

# ------------------------- LLM è¯·æ±‚å·¥å…· ------------------------- #

def _chat_with_ollama(system_prompt: str, user_prompt: str, *, model: str | None = None) -> str:
    """ä¸ Ollama é€šè®¯ï¼Œä¼˜å…ˆä½¿ç”¨ /api/chatï¼›è‹¥ 404/ä¸æ”¯æŒåˆ™å›é€€ /api/generateã€‚"""
    model_name = model or OLLAMA_MODEL
    chat_url = f"{OLLAMA_URL}/api/chat"
    chat_payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "options": {"temperature": 0.2, "num_predict": NUM_PREDICT},
        "stream": False,
    }

    try:
        resp = requests.post(chat_url, json=chat_payload, timeout=300)
        if resp.status_code == 404:
            raise RuntimeError("CHAT_NOT_SUPPORTED")
        resp.raise_for_status()
        data = resp.json()
        content = (data.get("message", {}) or {}).get("content", "").strip()
        if content:
            return content
        # è‹¥æ— å†…å®¹ï¼Œå°è¯•å›é€€ generate
        raise RuntimeError("EMPTY_CHAT_CONTENT")
    except Exception as e:
        # å›é€€åˆ° /api/generateï¼ˆæ—§ç‰ˆæœ¬ Ollama æˆ–ä¸æ”¯æŒ chatï¼‰
        if isinstance(e, RuntimeError) and str(e) in {"CHAT_NOT_SUPPORTED", "EMPTY_CHAT_CONTENT"} or (
            hasattr(e, "response") and getattr(e.response, "status_code", None) == 404
        ):
            gen_url = f"{OLLAMA_URL}/api/generate"
            # å°† system + user æ‹¼æˆå•æ¡ prompt
            prompt = f"[SYSTEM]\n{system_prompt}\n\n[USER]\n{user_prompt}"
            gen_payload = {
                "model": model_name,
                "prompt": prompt,
                "options": {"temperature": 0.2, "num_predict": NUM_PREDICT},
                "stream": False,
            }
            r2 = requests.post(gen_url, json=gen_payload, timeout=300)
            r2.raise_for_status()
            j2 = r2.json()
            content = (j2.get("response") or "").strip()
            return content
        logger.error("Ollama è¯·æ±‚å¤±è´¥: %s", str(e))
        raise


def _chat_with_openai(system_prompt: str, user_prompt: str) -> str:
    """ä¸ OpenAI å…¼å®¹ Chat API é€šè®¯ï¼Œè¿”å› assistant contentã€‚"""
    url = f"{OPENAI_BASE_URL.rstrip('/')}/chat/completions"
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": OPENAI_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": 0.2,
        "max_tokens": OPENAI_MAX_TOKENS,
        "stream": False,
    }
    try:
        resp = requests.post(url, headers=headers, json=payload, timeout=300)
        resp.raise_for_status()
        data = resp.json()
        content = data["choices"][0]["message"]["content"].strip()
        print(f"ğŸ” æœ€ç»ˆæ‹¼æ¥çš„ content: {content[:200]}...")
        return content
    except Exception as e:
        logger.error("OpenAI å…¼å®¹ API è¯·æ±‚å¤±è´¥: %s", e)
        raise

# ------------------------- å†…éƒ¨ç®—æ³• ------------------------- #

def _split_into_batches(lines: List[str], char_limit: int = BATCH_CHAR_LIMIT) -> List[List[str]]:
    """æŒ‰æ€»å­—ç¬¦æ•°å°†å­—å¹•è¡Œåˆ‡åˆ†ä¸ºå¤šæ‰¹ã€‚"""
    batches: List[List[str]] = []
    current: List[str] = []
    cur_len = 0
    for line in lines:  # type: ignore  
        add = len(line) + 1
        if current and cur_len + add > char_limit:
            batches.append(current)
            current = [line]
            cur_len = add
        else:
            current.append(line)
            cur_len += add
    if current:
        batches.append(current)
    return batches


def validate_single_translation(original: str, translated: str, target_lang: str) -> str:
    """
    éªŒè¯å•ä¸ªç¿»è¯‘ç»“æœçš„è´¨é‡
    
    Args:
        original: åŸæ–‡
        translated: ç¿»è¯‘ç»“æœ
        target_lang: ç›®æ ‡è¯­è¨€
    
    Returns:
        æœ‰æ•ˆçš„ç¿»è¯‘ç»“æœæˆ–åŸæ–‡ï¼ˆå¦‚æœç¿»è¯‘è´¨é‡ä¸ä½³ï¼‰
    """
    def _has_chinese(text: str) -> bool:
        return any('\u4e00' <= ch <= '\u9fff' for ch in text)
    
    # å¦‚æœç¿»è¯‘ç»“æœä¸ºç©ºï¼Œè¿”å›åŸæ–‡
    if not translated or not translated.strip():
        return original
    
    # å¦‚æœç›®æ ‡è¯­è¨€æ˜¯ä¸­æ–‡
    if target_lang.startswith("zh"):
        # æ£€æŸ¥ç¿»è¯‘ç»“æœæ˜¯å¦åŒ…å«ä¸­æ–‡
        if not _has_chinese(translated):
            return original
        
        # æ£€æŸ¥æ˜¯å¦åªæ˜¯é‡å¤äº†åŸæ–‡
        if translated.strip().lower() == original.strip().lower():
            return original
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„ç¿»è¯‘å¤±è´¥æ ‡å¿—
        failure_indicators = [
            "i cannot", "i can't", "sorry", "unable to", 
            "æ— æ³•ç¿»è¯‘", "ç¿»è¯‘å¤±è´¥", "error", "failed"
        ]
        if any(indicator in translated.lower() for indicator in failure_indicators):
            return original
    
    # ç¿»è¯‘ç»“æœé€šè¿‡éªŒè¯
    return translated


def _translate_batch(batch: List[str], target_lang: str) -> List[str]:
    """è°ƒç”¨ Ollama ç¿»è¯‘æ‰¹æ¬¡å­—å¹•ï¼Œè¿”å›é€è¡Œè¯‘æ–‡ã€‚"""
    
    def _has_chinese(text: str) -> bool:
        return any('\u4e00' <= ch <= '\u9fff' for ch in text)
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è‹±æ–‡å†…å®¹éœ€è¦ç¿»è¯‘
    has_english = any(any(c.isalpha() and ord(c) < 128 for c in text) for text in batch)
    if not has_english:
        logger.info("æ£€æµ‹åˆ°æ²¡æœ‰è‹±æ–‡å†…å®¹éœ€è¦ç¿»è¯‘ï¼Œä¿æŒåŸæ ·")
        return batch
    
    joined = f" {DELIM} ".join(batch)
    keep_terms_clause = (" Keep these terms exactly as-is: " + " | ".join(KEEP_TERMS) + ".") if KEEP_TERMS else ""
    # è°ƒæ•´æç¤ºï¼šä»…ä¸¥æ ¼ä¿ç•™æœ¯è¯­è¡¨ä¸ä»£ç /è·¯å¾„/ç¼©å†™ï¼Œé¿å…æ•´å¥è¢«è¯¯åˆ¤ä¸ºéœ€ä¿ç•™
    system_prompt = (
        "You are a professional subtitle translator. "
        f"Translate each segment into {target_lang}. Segments are separated by the token {DELIM}. "
        "Translate ONLY the natural-language parts; translate around preserved tokens. "
        "STRICTLY KEEP the following AS-IS: entries from the provided terminology list, inline code, file names/paths, "
        "CLI commands, and common acronyms (e.g., API, SDK, GPU). Preserve numbers and units. "
        "Do NOT add brackets or explanations; do NOT transliterate; preserve casing." + keep_terms_clause + " "
        f"Return EXACTLY the same number of segments, in the same order, separated by {DELIM} and nothing else."
    )

    print(f"ğŸ” å‘é€ç»™ Ollama:")
    print(f"   system: {system_prompt}")
    print(f"   user: {joined[:300]}...")
    print(f"   è¯·æ±‚é•¿åº¦: {len(joined)} å­—ç¬¦")
    
    try:
        # å‚è€ƒ KlicStudioï¼šæ”¯æŒä¸åŒ LLM Provider
        if TRANSLATE_PROVIDER == "openai" and OPENAI_API_KEY:
            content = _chat_with_openai(system_prompt, joined)
        else:
            content = _chat_with_ollama(system_prompt, joined)
        print(f"ğŸ” Ollama è¿”å› content: {content}")
        # æ‹†åˆ†è¯‘æ–‡ï¼ˆä¸ä¸¢å¼ƒç©ºæ®µï¼Œå°½é‡ä¿æŒä¸è¾“å…¥å¯¹é½ï¼‰
        translated_lines = [seg.strip() for seg in content.split(DELIM)]
        print(f"ğŸ” è§£æåè¯‘æ–‡ ({len(translated_lines)} æ¡): {translated_lines[:3]}...")

        # è‹¥ç›®æ ‡ä¸­æ–‡ä½†è¯‘æ–‡ä¸å«ä¸­æ–‡ï¼Œè§†ä¸ºå¤±è´¥
        if target_lang.startswith("zh") and not any(_has_chinese(t) for t in translated_lines):
            logger.warning("Ollama è¿”å›å†…å®¹ä¸å«ä¸­æ–‡ï¼Œé‡è¯•ä¸€æ¬¡â€¦")
            # å†æ¬¡å°è¯•ï¼Œé™„å¸¦æ›´å¼ºæŒ‡ä»¤
            retry_prompt = system_prompt + "\næ³¨æ„ï¼šåŠ¡å¿…è¾“å‡ºç®€ä½“ä¸­æ–‡ï¼Œä»…è¿”å›ç¿»è¯‘æ–‡æœ¬ã€‚"
            if TRANSLATE_PROVIDER == "openai" and OPENAI_API_KEY:
                content = _chat_with_openai(retry_prompt, joined)
            else:
                content = _chat_with_ollama(retry_prompt, joined)
            translated_lines = [seg.strip() for seg in content.split(DELIM)]

        # å¯¹é½æ®µæ•°ï¼šä¸ç›´æ¥ç”¨åŸæ–‡å¡«å……ï¼Œè€Œæ˜¯å…ˆå ä½ä¸ºç©ºï¼Œåç»­é€è¡Œé‡è¯•
        if len(translated_lines) != len(batch):
            logger.warning("è¯‘æ–‡è¡Œæ•°ä¸è¾“å…¥ä¸ä¸€è‡´ (in=%d, out=%d)ï¼Œå…ˆå¯¹é½é•¿åº¦å¹¶æ ‡è®°éœ€é‡è¯•ã€‚", len(batch), len(translated_lines))
            if len(translated_lines) < len(batch):
                translated_lines = translated_lines + [""] * (len(batch) - len(translated_lines))
            else:
                translated_lines = translated_lines[: len(batch)]

        # é€é¡¹æ ¡éªŒï¼šå¯¹ä¸å«ä¸­æ–‡/ç­‰äºåŸæ–‡/ä¸ºç©ºçš„æ¡ç›®è¿›è¡Œè¡Œçº§é‡è¯•ï¼ˆå«å¤‡ç”¨æ¨¡å‹ï¼‰
        def _is_valid_cn_item(src: str, hyp: str) -> bool:
            if not hyp or not hyp.strip():
                return False
            if hyp.strip().lower() == src.strip().lower():
                return False
            return _has_chinese(hyp)

        fixed_lines: list[str] = []
        for src_text, hyp_text in zip(batch, translated_lines):
            if target_lang.startswith("zh") and not _is_valid_cn_item(src_text, hyp_text):
                single_prompt = (
                    "You are a professional subtitle translator. "
                    f"Translate the following line into {target_lang}. Translate ONLY the natural-language parts; "
                    "keep terminology/code/paths/acronyms as-is. Return ONLY the translation text."
                )
                # ç¬¬ä¸€æ¬¡å•è¡Œé‡è¯•
                try:
                    ans = _chat_with_openai(single_prompt, src_text) if (TRANSLATE_PROVIDER == "openai" and OPENAI_API_KEY) else _chat_with_ollama(single_prompt, src_text)
                    ans = (ans or "").strip()
                except Exception:
                    ans = ""

                # è‹¥ä»æ— æ•ˆï¼Œé™„åŠ æ›´å¼ºé™åˆ¶ä¸­æ–‡æç¤º
                if not _is_valid_cn_item(src_text, ans):
                    try:
                        ans = _chat_with_openai(single_prompt + "\nåŠ¡å¿…è¾“å‡ºç®€ä½“ä¸­æ–‡ï¼Œä»…è¿”å›ç¿»è¯‘æ–‡æœ¬ã€‚", src_text) if (TRANSLATE_PROVIDER == "openai" and OPENAI_API_KEY) else _chat_with_ollama(single_prompt + "\nåŠ¡å¿…è¾“å‡ºç®€ä½“ä¸­æ–‡ï¼Œä»…è¿”å›ç¿»è¯‘æ–‡æœ¬ã€‚", src_text)
                        ans = (ans or "").strip()
                    except Exception:
                        ans = ""

                # è‹¥è¿˜ä¸è¡Œï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹
                if not _is_valid_cn_item(src_text, ans) and OLLAMA_FALLBACK_MODEL and TRANSLATE_PROVIDER != "openai":
                    try:
                        ans = _chat_with_ollama(single_prompt, src_text, model=OLLAMA_FALLBACK_MODEL)
                        ans = (ans or "").strip()
                    except Exception:
                        ans = ""

                fixed_lines.append(ans if _is_valid_cn_item(src_text, ans) else src_text)
            else:
                fixed_lines.append(hyp_text)
        translated_lines = fixed_lines

        # éªŒè¯æ¯ä¸ªç¿»è¯‘ç»“æœçš„è´¨é‡ï¼Œå¯¹ç¿»è¯‘å¤±è´¥çš„ä¿ç•™åŸæ–‡
        validated_translations = []
        for i, (original, translated) in enumerate(zip(batch, translated_lines)):
            validated_translation = validate_single_translation(original, translated, target_lang)
            validated_translations.append(validated_translation)
            
            if validated_translation == original:
                logger.debug(f"ç¿»è¯‘æ¡ç›® {i+1} è´¨é‡ä¸ä½³ï¼Œä¿ç•™åŸæ–‡: {original[:30]}...")

        return validated_translations
    except Exception as e:
        logger.error("ç¿»è¯‘æ‰¹æ¬¡å¤±è´¥: %s", e)
        raise Exception(f"ç¿»è¯‘æ‰¹æ¬¡å¤±è´¥: {e}")

# ------------------------- å¯¹å¤–ä¸»æ¥å£ ------------------------- #

def translate_srt_to_zh(srt_path: str, target_lang: str = "zh", **kwargs) -> str:
    """ç¿»è¯‘ SRT æ–‡ä»¶ï¼Œè¿”å›ç¿»è¯‘åä¸´æ—¶æ–‡ä»¶è·¯å¾„ã€‚"""
    logger.info("å¼€å§‹ç¿»è¯‘å­—å¹• %s -> %s (Ollama)", srt_path, target_lang)
        
    # è¯»å–å¹¶è§£æåŸå­—å¹•
    with open(srt_path, "r", encoding="utf-8") as fp:
        subs = list(srt.parse(fp.read()))
            
    if TRANSLATE_LINE_BY_LINE:
        logger.info("å¯ç”¨é€å¥ç¿»è¯‘æ¨¡å¼ï¼ˆä¸åˆ†æ‰¹ï¼‰â€¦")
        translated: List[str] = []
        keep_terms_clause = (" Keep these terms exactly as-is: " + " | ".join(KEEP_TERMS) + ".") if KEEP_TERMS else ""
        for i, sub in enumerate(subs, 1):
            line = sub.content.replace("\n", " ")
            system_prompt = (
                "You are a professional subtitle translator. "
                f"Translate the following line into {target_lang}. Translate ONLY the natural-language parts; "
                "STRICTLY KEEP entries from the terminology list, inline code, file names/paths, CLI commands, and common acronyms AS-IS. "
                "Preserve numbers and units. Do NOT add brackets or explanations; do NOT transliterate; preserve casing." + keep_terms_clause + " "
                "Return ONLY the translation text."
            )
            try:
                ans = _chat_with_openai(system_prompt, line) if (TRANSLATE_PROVIDER == "openai" and OPENAI_API_KEY) else _chat_with_ollama(system_prompt, line)
                ans = (ans or "").strip()
                if target_lang.startswith("zh") and not any('\u4e00' <= ch <= '\u9fff' for ch in ans):
                    # åŠ å¼ºä¸€æ¬¡é‡è¯•
                    try:
                        ans2 = _chat_with_openai(system_prompt + "\nåŠ¡å¿…è¾“å‡ºç®€ä½“ä¸­æ–‡ï¼Œä»…è¿”å›ç¿»è¯‘æ–‡æœ¬ã€‚", line) if (TRANSLATE_PROVIDER == "openai" and OPENAI_API_KEY) else _chat_with_ollama(system_prompt + "\nåŠ¡å¿…è¾“å‡ºç®€ä½“ä¸­æ–‡ï¼Œä»…è¿”å›ç¿»è¯‘æ–‡æœ¬ã€‚", line)
                        ans2 = (ans2 or "").strip()
                        if any('\u4e00' <= ch <= '\u9fff' for ch in ans2):
                            ans = ans2
                        elif OLLAMA_FALLBACK_MODEL and TRANSLATE_PROVIDER != "openai":
                            ans3 = _chat_with_ollama(system_prompt, line, model=OLLAMA_FALLBACK_MODEL)
                            ans3 = (ans3 or "").strip()
                            ans = ans3 if any('\u4e00' <= ch <= '\u9fff' for ch in ans3) else line
                        else:
                            ans = line
                    except Exception:
                        ans = line
            except Exception:
                ans = line
            translated.append(validate_single_translation(line, ans, target_lang))
        logger.info("é€å¥ç¿»è¯‘å®Œæˆï¼š%d è¡Œ", len(translated))
    else:
        texts = [s.content.replace("\n", " ") for s in subs]
        batches = _split_into_batches(texts)
        logger.info("å…± %d è¡Œå­—å¹•ï¼Œæ‹†ä¸º %d æ‰¹", len(texts), len(batches))

        translated = []
        done = 0
        for idx, batch in enumerate(batches, 1):
            logger.info("ç¿»è¯‘æ‰¹æ¬¡ %d/%d (â‰ˆ%d è¡Œ)â€¦", idx, len(batches), len(batch))
            translated.extend(_translate_batch(batch, target_lang))
            done += len(batch)
            logger.info("å·²ç¿»è¯‘ %d/%d è¡Œ", done, len(texts))

    if len(translated) != len(subs):
        logger.error("ç¿»è¯‘åè¡Œæ•°ä¸åŒ¹é…ï¼Œç¿»è¯‘å¤±è´¥: %s", srt_path)
        raise Exception(f"ç¿»è¯‘å¤±è´¥ï¼šæœŸæœ› {len(subs)} è¡Œï¼Œå®é™…å¾—åˆ° {len(translated)} è¡Œ")

    for sub, new_txt in zip(subs, translated):
        sub.content = new_txt

    zh_content = srt.compose(subs)
    with tempfile.NamedTemporaryFile("w", suffix=".zh.srt", delete=False, encoding="utf-8") as fp:
        fp.write(zh_content)
        out_path = fp.name

    logger.info("å­—å¹•ç¿»è¯‘å®Œæˆï¼š%s", out_path)
    return out_path


def translate_srt_to_bilingual(en_srt_path: str, target_lang: str = "zh", **kwargs) -> str:
    """
    ç¿»è¯‘SRTæ–‡ä»¶å¹¶ç”ŸæˆåŒè¯­å­—å¹•
    
    Args:
        en_srt_path: è‹±æ–‡å­—å¹•æ–‡ä»¶è·¯å¾„
        target_lang: ç›®æ ‡è¯­è¨€
    
    Returns:
        åŒè¯­å­—å¹•æ–‡ä»¶è·¯å¾„
    """
    logger.info("å¼€å§‹ç”ŸæˆåŒè¯­å­—å¹• %s -> %s (Ollama)", en_srt_path, target_lang)
    
    # å…ˆè¿›è¡Œæ­£å¸¸ç¿»è¯‘
    zh_srt_path = translate_srt_to_zh(en_srt_path, target_lang, **kwargs)
    
    # å¯¼å…¥åŒè¯­å­—å¹•åˆå¹¶æ¨¡å—
    try:
        from .bilingual_subtitle_merger import create_bilingual_subtitles_from_translation
        
        # ç”ŸæˆåŒè¯­å­—å¹•
        bilingual_srt_path = create_bilingual_subtitles_from_translation(
            en_srt_path, zh_srt_path
        )
        
        logger.info("åŒè¯­å­—å¹•ç”Ÿæˆå®Œæˆ: %s", bilingual_srt_path)
        return bilingual_srt_path
        
    except ImportError as e:
        logger.error("æ— æ³•å¯¼å…¥åŒè¯­å­—å¹•åˆå¹¶æ¨¡å—: %s", e)
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œè¿”å›ä¸­æ–‡å­—å¹•
        return zh_srt_path
    except Exception as e:
        logger.error("ç”ŸæˆåŒè¯­å­—å¹•å¤±è´¥: %s", e)
        # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œè¿”å›ä¸­æ–‡å­—å¹•
        return zh_srt_path


def translate_text(text: str, target_lang: str = "zh") -> str:
    """ç¿»è¯‘ä»»æ„æ–‡æœ¬çš„ä¾¿æ·æ–¹æ³•ã€‚"""
    return _translate_batch([text], target_lang)[0]


# å‘åå…¼å®¹ main.py æ—©æœŸæ¥å£ -----------------------------------------------------

def translate_video_title(title: str, target_lang: str = "zh") -> str:
    """æ—§ç‰ˆæ¥å£åŒ…è£…ï¼šç¿»è¯‘è§†é¢‘æ ‡é¢˜ã€‚"""
    return translate_text(title, target_lang)
